{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6jtkGmDPm1m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch_geometric import seed_everything\n",
        "from torch_geometric.nn import GCNConv, GATConv, GraphConv, MFConv\n",
        "from torch_geometric.nn import Linear\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import QM9\n",
        "from torch_geometric.data import Batch\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from permetrics.regression import RegressionMetric\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MolFromSmiles, SDWriter, Draw\n",
        "from rdkit.Chem import AllChem, AddHs\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
        "from tqdm import tqdm\n",
        "import learn2learn as l2l\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "seed_everything(42)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def get_key_by_value(dictionary, value):\n",
        "    for key, val in dictionary.items():\n",
        "        if val == value:\n",
        "            return key\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Y3Jkck_0Mqm0",
        "outputId": "8c48e59f-7acd-4ca0-e5ff-f5ad99398608"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('./henry.xlsx')\n",
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "# keep only samples from df that have df['temps'] around 298\n",
        "df = df[np.abs(df['temps'] - 298) < 5]\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# keep only tasks which have more than hiper_batch_size samples\n",
        "df = df.groupby('smiles_solutes').filter(lambda x: len(x) > 4)\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df['Hcs'] = df['Hcs'] * 1000\n",
        "\n",
        "# remove O=C=O samples\n",
        "df = df[df['smiles_solutes'] != 'O=C=O']\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# load into df2 another excel file\n",
        "df2 = pd.read_csv('carbon-dioxide.csv')\n",
        "df2 = df2[df2['Split'] != 0]\n",
        "df2 = df2.reset_index(drop=True)\n",
        "\n",
        "df2['smiles_solutes'] = 'O=C=O'\n",
        "\n",
        "pre_defined_split = False\n",
        "\n",
        "test_indices = train_test_split(df2.index, test_size=0.2, random_state=0)[1]\n",
        "df2.loc[test_indices, 'smiles_solutes'] = 'O=C=O_test'\n",
        "\n",
        "df2.rename(columns={\"Henry's law constant\": 'Hcs', \"Temperature\": 'temps',\n",
        "                    \"SMILES\": \"smiles\"}, inplace=True)\n",
        "df2 = df2[['smiles', 'smiles_solutes', 'temps', 'Hcs']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df2['Hcs'].values.reshape(-1, 1))\n",
        "df2['Hcs'] = scaler.transform(df2['Hcs'].values.reshape(-1, 1))\n",
        "df['Hcs'] = scaler.transform(df['Hcs'].values.reshape(-1, 1))\n",
        "\n",
        "# join df and df2\n",
        "df = pd.concat([df, df2])\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# expand df['smiles'] based on dot into smiles_cation and smiles_anion\n",
        "df['smiles_cation'] = ''\n",
        "df['smiles_anion'] = ''\n",
        "for i, row in df.iterrows():\n",
        "    smiles = row['smiles']\n",
        "    temp1, temp2 = smiles.split('.')\n",
        "    if '+' in temp1:\n",
        "        df.at[i, 'smiles_cation'] = temp1\n",
        "        df.at[i, 'smiles_anion'] = temp2\n",
        "    else:\n",
        "        df.at[i, 'smiles_cation'] = temp2\n",
        "        df.at[i, 'smiles_anion'] = temp1\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9dIEl8DPUY8",
        "outputId": "0c58a331-69cc-4c97-8908-ec41a2ed1c69"
      },
      "outputs": [],
      "source": [
        "df3 = pd.read_excel('./cosmo-predicted-hcs.xlsx')\n",
        "df3['ils'] = df3['IL_cation'] + ' ' + df3['IL_anion']\n",
        "\n",
        "# remove rows where henrycnodim == 0\n",
        "df3 = df3[df3['henrycnodim'] != 0]\n",
        "df3 = df3[df3['henryc'] != 0]\n",
        "df3['henryc'] = df3['henryc'] * 101325 # units alignment\n",
        "df3 = df3.reset_index(drop=True)\n",
        "\n",
        "target_column_cosmo = 'henryc'\n",
        "df3[target_column_cosmo] = 1 / (df3[target_column_cosmo])\n",
        "df3['smiles'] = df3['smiles_cation'] + '.' + df3['smiles_anion']\n",
        "df3['temps'] = 298\n",
        "df3['smiles_solutes'] = df3['task'] + '_cosmo'\n",
        "df3 = df3.dropna(subset=['smiles_anion'])\n",
        "df3 = df3.reset_index(drop=True)\n",
        "df3 = df3[['smiles', 'smiles_cation', 'smiles_anion', 'temps', target_column_cosmo, 'ils', 'smiles_solutes']]\n",
        "df3.rename(columns={target_column_cosmo: 'Hcs'}, inplace=True)\n",
        "q1 = df3['Hcs'].quantile(0.25)\n",
        "q3 = df3['Hcs'].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "df3 = df3[(df3['Hcs'] >= q1 - 1.5 * iqr) & (df3['Hcs'] <= q3 + 1.5 * iqr)]\n",
        "df3 = df3.reset_index(drop=True)\n",
        "\n",
        "scaler3 = StandardScaler()\n",
        "df3['Hcs'] = scaler3.fit_transform(df3['Hcs'].values.reshape(-1, 1))\n",
        "df = pd.concat([df, df3])\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Rmh9ab78QYgP",
        "outputId": "4521833a-2f07-4a4c-9202-6170e181059a"
      },
      "outputs": [],
      "source": [
        "nrm = rdMolStandardize.Normalizer()\n",
        "\n",
        "def normalize_smiles(smile):\n",
        "  cosmo_flag = False\n",
        "  if '_cosmo' in smile:\n",
        "    smile = smile.replace('_cosmo', '')\n",
        "    cosmo_flag = True\n",
        "  mol = Chem.MolFromSmiles(smile)\n",
        "  mol_norm = nrm.normalize(mol)\n",
        "  smile_norm = Chem.MolToSmiles(mol_norm, True)\n",
        "  if cosmo_flag:\n",
        "    smile_norm = smile_norm + '_cosmo'\n",
        "  return smile_norm\n",
        "\n",
        "smiles_cation_unique = df['smiles_cation'].unique()\n",
        "smiles_anion_unique = df['smiles_anion'].unique()\n",
        "\n",
        "smiles_cation_norm_dict = {}\n",
        "smiles_anion_norm_dict = {}\n",
        "for smile in smiles_cation_unique:\n",
        "  smiles_cation_norm_dict[smile] = normalize_smiles(smile)\n",
        "for smile in smiles_anion_unique:\n",
        "  smiles_anion_norm_dict[smile] = normalize_smiles(smile)\n",
        "\n",
        "df['smiles_cation'] = df['smiles_cation'].map(smiles_cation_norm_dict)\n",
        "df['smiles_anion'] = df['smiles_anion'].map(smiles_anion_norm_dict)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlQ_uvf-QY_r"
      },
      "outputs": [],
      "source": [
        "fold_to_use = 3\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "task = 'O=C=O'\n",
        "\n",
        "# remove sufix '_valid' from samples in df\n",
        "df['smiles_solutes'] = df['smiles_solutes'].str.replace('_valid', '')\n",
        "\n",
        "# randomly select number of percentage of samples with this task\n",
        "df_task = df[df['smiles_solutes'] == task]\n",
        "\n",
        "for fold_index, (train_index, valid_index) in enumerate(kf.split(df_task)):\n",
        "  if fold_index == fold_to_use:\n",
        "    df.loc[df_task.iloc[valid_index].index, 'smiles_solutes'] = task + f'_valid'\n",
        "    break  # Exit after marking the specified fold as validation\n",
        "\n",
        "# Extract tasks\n",
        "tasks = df['smiles_solutes'].values\n",
        "tasks_dict = {task: i for i, task in enumerate(np.unique(tasks))}\n",
        "tasks = np.array([tasks_dict[task] for task in tasks])\n",
        "\n",
        "y = df['Hcs'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqZDESHRQp41",
        "outputId": "52d6cce7-4aee-476b-8d7d-75328d2c57c6"
      },
      "outputs": [],
      "source": [
        "def get_atom_features(smi):\n",
        "  mol = Chem.MolFromSmiles(smi)\n",
        "  mol = Chem.RemoveHs(mol)\n",
        "  atomic_number = []\n",
        "  num_hs = []\n",
        "  hybr = []\n",
        "  charges = []\n",
        "  aromacity = []\n",
        "  degrees = []\n",
        "\n",
        "  for i, atom in enumerate(mol.GetAtoms()):\n",
        "      atomic_number.append(atom.GetAtomicNum())\n",
        "      num_hs.append(atom.GetTotalNumHs(includeNeighbors=True))\n",
        "      hybr.append(atom.GetHybridization())\n",
        "\n",
        "      charge = atom.GetFormalCharge()\n",
        "      if np.isnan(charge):\n",
        "          charge = 0\n",
        "      charges.append(charge)\n",
        "\n",
        "      aromacity.append(atom.GetIsAromatic())\n",
        "      degrees.append(atom.GetDegree())\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  hybr = le.fit_transform(hybr)\n",
        "\n",
        "  result = np.array([atomic_number, num_hs, hybr, charges, aromacity, degrees])\n",
        "  return np.transpose(result)\n",
        "\n",
        "def get_edges_info(smi):\n",
        "  mol = Chem.MolFromSmiles(smi)\n",
        "  mol = Chem.RemoveHs(mol)\n",
        "  row, col, bonds_types = [], [], []\n",
        "\n",
        "  for i, bond in enumerate(mol.GetBonds()):\n",
        "      start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
        "      row += [start, end]\n",
        "      col += [end, start]\n",
        "      bonds_types += [bond.GetBondTypeAsDouble(), bond.GetBondTypeAsDouble()]\n",
        "  return np.array([row, col], dtype=np.int_), np.array(bonds_types, dtype=np.float32)\n",
        "\n",
        "memo = dict()\n",
        "\n",
        "def dataframe_row_into_pytorch_geometric_molecular_graph(row, memo):\n",
        "  if row['smiles'] in memo:\n",
        "    x, edge_index, edge_attr = memo[row['smiles']]\n",
        "  else:\n",
        "    #calculate x for c and a\n",
        "    c_x = get_atom_features(row['smiles_cation'])\n",
        "    a_x = get_atom_features(row['smiles_anion'])\n",
        "\n",
        "    temp_up = np.concatenate([c_x, np.zeros([c_x.shape[0],a_x.shape[1]])], axis=1)\n",
        "    temp_down = np.concatenate([np.zeros([a_x.shape[0],c_x.shape[1]]), a_x], axis=1)\n",
        "    b = np.concatenate([temp_up, temp_down]) #b for both c and a\n",
        "    x = torch.tensor(b, dtype=torch.float)\n",
        "\n",
        "    #calculate edges for c and a\n",
        "    c_edge_index, c_edge_weights = get_edges_info(row['smiles_cation'])\n",
        "    a_edge_index, a_edge_weights = get_edges_info(row['smiles_anion'])\n",
        "\n",
        "    b = np.concatenate([c_edge_index, a_edge_index], axis=1)\n",
        "    edge_index = torch.tensor(b, dtype=torch.long)\n",
        "\n",
        "    b = np.concatenate([c_edge_weights, a_edge_weights])\n",
        "    edge_attr = torch.tensor(b, dtype=torch.float)\n",
        "\n",
        "    memo[row['smiles']] = (x, edge_index, edge_attr)\n",
        "  y = torch.tensor(row['Hcs'], dtype=torch.float)\n",
        "  id = row['ids']\n",
        "  # ffv = torch.tensor(row['FFV'], dtype=torch.float)\n",
        "  data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
        "              y=y, id=id, smiles=row['smiles'],\n",
        "              # ffv=ffv\n",
        "              )\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "5XYmgbAHQsqh",
        "outputId": "3a55896a-8bd4-4c4c-8625-99b13f1672b3"
      },
      "outputs": [],
      "source": [
        "df['ids'] = tasks\n",
        "graphs_list = []\n",
        "for row in df.iterrows():\n",
        "  row = row[1]\n",
        "  data = dataframe_row_into_pytorch_geometric_molecular_graph(row, memo)\n",
        "  graphs_list.append(data)\n",
        "df['graphs'] = graphs_list\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5_bb2MbO8Nf",
        "outputId": "95a2747c-a992-4ddc-b0c0-5a901d310c30"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  ids = [data.id for data in batch]\n",
        "  unique_ids = sorted(list(set(ids)))\n",
        "  batches = []\n",
        "  for id_ in unique_ids:\n",
        "    same_id_data = [data for data in batch if data.id == id_]\n",
        "    batches.append(Batch.from_data_list(same_id_data))\n",
        "  return batches\n",
        "\n",
        "# preapre loaders using this function\n",
        "train_loader = DataLoader(df[~df['smiles_solutes'].isin(('O=C=O', 'O=C=O_valid', 'O=C=O_test'))]['graphs'].values, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "adapt_loader = DataLoader(df[df['smiles_solutes'] == 'O=C=O']['graphs'].values, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(df[df['smiles_solutes'] == 'O=C=O_valid']['graphs'].values, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(df[df['smiles_solutes'] == 'O=C=O_test']['graphs'].values, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAGcec2LPhTC"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, conv_function, input_channels, embedding_size, linear_size, add_params_num=0):\n",
        "        # Init parent\n",
        "        super(GCN, self).__init__()\n",
        "        self.crafted_add_params_num = add_params_num\n",
        "\n",
        "        # GCN layers\n",
        "        self.conv1 = conv_function(input_channels, embedding_size[0])\n",
        "        self.conv2 = conv_function(embedding_size[0], embedding_size[1])\n",
        "        self.conv3 = conv_function(embedding_size[1], embedding_size[2])\n",
        "        self.conv4 = conv_function(embedding_size[2], embedding_size[3])\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout1 = torch.nn.Dropout(0.2)\n",
        "\n",
        "        # Linear layers\n",
        "        self.linear1 = Linear(embedding_size[-1]+add_params_num, linear_size[0])\n",
        "        self.linear2 = Linear(linear_size[0],linear_size[1])\n",
        "\n",
        "        # Dropout 2\n",
        "        self.dropout2 = torch.nn.Dropout(0.3)\n",
        "\n",
        "        # batch normalization\n",
        "        self.bnf = torch.nn.BatchNorm1d(linear_size[-1])\n",
        "\n",
        "        # Output layer\n",
        "        self.out = Linear(linear_size[-1], 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight, batch_index, cond=None):\n",
        "        # Conv layers\n",
        "        hidden = self.conv1(x, edge_index, edge_weight).relu()\n",
        "        hidden = self.dropout1(hidden)\n",
        "        hidden = self.conv2(hidden, edge_index, edge_weight).relu()\n",
        "        hidden = self.dropout1(hidden)\n",
        "        hidden = self.conv3(hidden, edge_index, edge_weight).relu()\n",
        "        hidden = self.dropout1(hidden)\n",
        "        hidden = self.conv4(hidden, edge_index, edge_weight).relu()\n",
        "        hidden = self.dropout1(hidden)\n",
        "\n",
        "        # Pooling\n",
        "        hidden = gap(hidden, batch_index)\n",
        "\n",
        "        # adding pressure and temperature info\n",
        "        if self.crafted_add_params_num != 0:\n",
        "            cond = cond.unsqueeze(1)\n",
        "            hidden = torch.cat([hidden, cond], dim=1)\n",
        "\n",
        "        # Apply a final (linear) classifier.\n",
        "        hidden = self.linear1(hidden)\n",
        "        hidden = self.linear2(hidden)\n",
        "        hidden = self.dropout2(hidden)\n",
        "        hidden = self.bnf(hidden)\n",
        "        hidden = torch.nn.functional.relu(hidden)\n",
        "        out = self.out(hidden)\n",
        "\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38b75Oh0RHwA"
      },
      "outputs": [],
      "source": [
        "cond_names = [] #['FFV']\n",
        "\n",
        "GCN_model = GCN(GCNConv, input_channels = df['graphs'].values[0].x.shape[1],\n",
        "            embedding_size = [128,256,256,128], linear_size = [256,128],\n",
        "            add_params_num = len(cond_names))\n",
        "\n",
        "maml = l2l.algorithms.MAML(GCN_model, lr=1e-3).to(device)\n",
        "optimizer_maml = torch.optim.Adam(maml.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_maml, 40)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqyGDtEIS1rv",
        "outputId": "ddf9cdbf-566f-4f6b-824e-45ff09cc683d"
      },
      "outputs": [],
      "source": [
        "epochs = 30\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "model = maml.clone().to(device)\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    r2 = 0\n",
        "    for data in train_loader:\n",
        "\n",
        "        model = maml.clone().to(device)\n",
        "\n",
        "        data = data.to(device)\n",
        "        optimizer_maml.zero_grad()\n",
        "        if cond_names:\n",
        "            out, _ = model(data.x, data.edge_index, data.edge_attr, data.batch, data.ffv)\n",
        "        else:\n",
        "            out, _ = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = criterion(out.squeeze(), data.y)\n",
        "\n",
        "    model.adapt(loss)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer_maml.step()\n",
        "    scheduler.step()\n",
        "    train_loss += loss.detach().cpu().item()\n",
        "    train_loss /= len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XOwZYyYVK56",
        "outputId": "3b0bbcb3-571f-46db-a415-ca25ce4b9ac6"
      },
      "outputs": [],
      "source": [
        "model = maml.clone()\n",
        "state_dict = model.module.state_dict()\n",
        "model_to_adapt = GCN(GCNConv, input_channels = df['graphs'].values[0].x.shape[1],\n",
        "            embedding_size = [128,256,256,128], linear_size = [256,128],\n",
        "            add_params_num = len(cond_names))\n",
        "model_to_adapt.load_state_dict(state_dict)\n",
        "model_to_adapt = model_to_adapt.to(device)\n",
        "optimizer = torch.optim.Adam(model_to_adapt.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 40)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "epochs = 300\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "r2s_valid = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "train_loss, valid_loss, r2 = 0, 0, 0\n",
        "\n",
        "for epoch in (pbar := tqdm(range(epochs))):\n",
        "    pbar.set_description(f\"Loss: {train_loss:.3f}, valid_loss: {valid_loss:.3f}, R2: {r2:0.3f}\")\n",
        "\n",
        "    model_to_adapt.train()\n",
        "    train_loss = 0\n",
        "    r2 = 0\n",
        "    for data in adapt_loader:\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        if cond_names:\n",
        "            out, _ = model_to_adapt(data.x, data.edge_index, data.edge_attr, data.batch, data.ffv)\n",
        "        else:\n",
        "            out, _ = model_to_adapt(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = criterion(out.squeeze(), data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        train_loss += loss.detach().cpu().item()\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    model_to_adapt.eval()\n",
        "    valid_loss = 0\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            data = data.to(device)\n",
        "            if cond_names:\n",
        "                out, _ = model_to_adapt(data.x, data.edge_index, data.edge_attr, data.batch, data.ffv)\n",
        "            else:\n",
        "                out, _ = model_to_adapt(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            loss = criterion(out.squeeze(), data.y)\n",
        "            valid_loss += loss.detach().cpu().item()\n",
        "            y_true.extend(data.y.cpu().numpy())\n",
        "            y_pred.extend(out.squeeze().cpu().numpy())\n",
        "    valid_loss /= len(valid_loader)\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    # Calculate metrics using permetrics\n",
        "    metric = RegressionMetric(y_true, y_pred)\n",
        "    rmse = metric.get_metric_by_name('RMSE')['RMSE']\n",
        "    r2 = metric.get_metric_by_name('R2')['R2']\n",
        "    mae = metric.get_metric_by_name('MAE')['MAE']\n",
        "    mare = metric.get_metric_by_name('MAPE')['MAPE']\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    r2s_valid.append(r2)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model_to_adapt.state_dict(), 'best_model.pth')\n",
        "        best_train_loss = train_loss\n",
        "        best_valid_loss = valid_loss\n",
        "        best_r2 = r2\n",
        "        best_rmse = rmse\n",
        "        best_mae = mae\n",
        "        best_mare = mare\n",
        "\n",
        "print(f'\\nTrain Loss: {best_train_loss:.4f} | Valid Loss: {best_valid_loss:.4f}', end=' | ')\n",
        "print(f'RMSE: {best_rmse:.4f} | R2: {best_r2:.4f} | MAE: {best_mae:.4f} | MARE: {best_mare:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "M0BlLRifYMax",
        "outputId": "6234ff13-c8f7-4435-96b2-85221e1cd2e7"
      },
      "outputs": [],
      "source": [
        "# plot train_losses vs epoch\n",
        "plt.figure(figsize=(15,4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# plot another plot with r2 vs epoch\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(r2s_valid, label='R2')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('R2')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kES_NNt3UMcS",
        "outputId": "5e32f257-8df7-4e4b-d5fa-22c8619ffd66"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model_to_adapt.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# Function to perform testing and print metrics\n",
        "def test_model(loader, name):\n",
        "    model_to_adapt.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(device)\n",
        "            if cond_names:\n",
        "                out, _ = model_to_adapt(data.x, data.edge_index, data.edge_attr, data.batch, data.ffv)\n",
        "            else:\n",
        "                out, _ = model_to_adapt(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "            y_true.extend(data.y.cpu().numpy())\n",
        "            y_pred.extend(out.squeeze().cpu().numpy())\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "\n",
        "    metric = RegressionMetric(y_true, y_pred)\n",
        "    rmse = metric.get_metric_by_name('RMSE')['RMSE']\n",
        "    r2 = metric.get_metric_by_name('R2')['R2']\n",
        "    mae = metric.get_metric_by_name('MAE')['MAE']\n",
        "    mare = metric.get_metric_by_name('MAPE')['MAPE']\n",
        "\n",
        "    # print(f\"{name} Set Metrics: R2, RMSE, MAE, MARE\")\n",
        "    print(f\"{r2:.4f}\")\n",
        "    print(f\"{rmse:.4f}\")\n",
        "    print(f\"{mae:.4f}\")\n",
        "    print(f\"{mare:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Perform testing on train, validation, and test sets\n",
        "print(\"Training - Validation - Test : R2, RMSE, MAE, MARE\")\n",
        "test_model(adapt_loader, \"Training\")\n",
        "test_model(valid_loader, \"Validation\")\n",
        "test_model(test_loader, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edKRqyT7VIGq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
