{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01uvxIvEi_OA",
        "outputId": "27f8761f-6250-4bb8-d847-e5987886cccc"
      },
      "outputs": [],
      "source": [
        "import deepchem as dc\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from deepchem.metalearning.torch_maml import MetaLearner, MAML\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch_geometric import seed_everything\n",
        "from torch_geometric.nn import GCNConv, Linear\n",
        "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import QM9\n",
        "from tabulate import tabulate\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from permetrics.regression import RegressionMetric\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MolFromSmiles, SDWriter, Draw\n",
        "from rdkit.Chem import AllChem, AddHs\n",
        "from rdkit.Chem import rdFingerprintGenerator\n",
        "from rdkit.Chem.MolStandardize import rdMolStandardize\n",
        "from tqdm import tqdm\n",
        "\n",
        "seed_everything(42)\n",
        "\n",
        "def get_key_by_value(dictionary, value):\n",
        "    for key, val in dictionary.items():\n",
        "        if val == value:\n",
        "            return key\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yvo4-N5UD0c"
      },
      "outputs": [],
      "source": [
        "class MetaMFLearner(MetaLearner):\n",
        "    def __init__(self, layer_sizes=[1, 40, 20, 1], activation=F.relu, dataset=None, batch_size=10, tasks_dict=None, test_tasks=None):\n",
        "        self.batch_size = batch_size\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation = activation\n",
        "        self.dataset = dataset\n",
        "        self.task_index = None  # stores the current task ID\n",
        "        self.tasks_dict = tasks_dict\n",
        "        self.test_tasks = test_tasks\n",
        "        self.layers = self._create_layers()\n",
        "        self.train_indices = []\n",
        "\n",
        "    def _create_layers(self):\n",
        "        layers = []\n",
        "        for i in range(len(self.layer_sizes) - 1):\n",
        "            layers.append(torch.nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1]))\n",
        "        return torch.nn.ModuleList(layers)\n",
        "\n",
        "    def compute_model(self, inputs, variables, training):\n",
        "        device = next(self.parameters()).device\n",
        "        x, y = inputs\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i < len(self.layers) - 1:\n",
        "                x = self.activation(layer(x))\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        loss = torch.mean(torch.square(x - y))\n",
        "        return loss, [x]\n",
        "\n",
        "    @property\n",
        "    def variables(self):\n",
        "        return [param for layer in self.layers for param in layer.parameters()]\n",
        "\n",
        "    def select_task(self):\n",
        "        # Select a task ID from the unique IDs in the dataset\n",
        "        unique_ids = self.tasks_dict.values()\n",
        "        unique_ids = [x for x in unique_ids if x not in self.test_tasks]\n",
        "        self.task_index = random.choice(unique_ids)\n",
        "\n",
        "    def select_task_by_name(self, task_name):\n",
        "        self.task_index = self.tasks_dict[task_name]\n",
        "\n",
        "    # Fingerprints\n",
        "    def get_batch(self):\n",
        "        # Get samples belonging to the selected task ID\n",
        "        task_indices = np.where(self.dataset.ids == self.task_index)[0]\n",
        "        if len(task_indices) < self.batch_size:\n",
        "            batch_indices = np.random.choice(task_indices, len(task_indices), replace=False)\n",
        "        else:\n",
        "            batch_indices = np.random.choice(task_indices, self.batch_size, replace=False)\n",
        "        self.train_indices = batch_indices\n",
        "        x = torch.tensor(self.dataset.X[batch_indices], dtype=torch.float32)\n",
        "        y = torch.tensor(self.dataset.y[batch_indices], dtype=torch.float32).view(-1, 1)\n",
        "        return [x, y]\n",
        "\n",
        "    def parameters(self):\n",
        "        for param in self.variables:\n",
        "            yield param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "WRqQ8FezmBH6",
        "outputId": "cf2a6704-6f00-472e-e2c9-bb1c1d5beb8f"
      },
      "outputs": [],
      "source": [
        "hiper_batch_size = 32\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel('./henry.xlsx')\n",
        "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "# keep only samples from df that have df['temps'] around 298\n",
        "df = df[np.abs(df['temps'] - 298) < 5]\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df['Hcs'] = df['Hcs'] * 1000 # units adjustment\n",
        "\n",
        "df['smiles_solutes'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "x9p2yf2O0y9r",
        "outputId": "fdbfca96-b30a-45f0-e1f8-affa0a09b169"
      },
      "outputs": [],
      "source": [
        "# remove O=C=O samples (if any)\n",
        "df = df[df['smiles_solutes'] != 'O=C=O']\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# load into df2 another excel file\n",
        "df2 = pd.read_csv('carbon-dioxide.csv')\n",
        "df2.rename(columns={\"Henry's law constant\": 'Hcs', \"Temperature\": 'temps',\n",
        "                    \"SMILES\": \"smiles\"}, inplace=True)\n",
        "\n",
        "df2 = df2[df2['Split'] != 0]\n",
        "df2['smiles_solutes'] = 'O=C=O'\n",
        "\n",
        "test_indices = train_test_split(df2.index, test_size=0.2, random_state=0)[1]\n",
        "df2.loc[test_indices, 'smiles_solutes'] = 'O=C=O_test'\n",
        "\n",
        "df2 = df2[['smiles', 'smiles_solutes', 'temps', 'Hcs']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(df2['Hcs'].values.reshape(-1, 1))\n",
        "df2['Hcs'] = scaler.transform(df2['Hcs'].values.reshape(-1, 1))\n",
        "df['Hcs'] = scaler.fit_transform(df['Hcs'].values.reshape(-1, 1))\n",
        "\n",
        "# drop from df rows with df['Hcs'] value outside (-3,3)\n",
        "df = df[(df['Hcs'] >= -3) & (df['Hcs'] <= 3)]\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# join df and df2\n",
        "df = pd.concat([df, df2])\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "1WDjtV6Os0iU",
        "outputId": "8eecaf7e-4909-42f2-a283-16dd496c3621"
      },
      "outputs": [],
      "source": [
        "# expand df['smiles'] based on dot into smiles_cation and smiles_anion\n",
        "df['smiles_cation'] = ''\n",
        "df['smiles_anion'] = ''\n",
        "for i, row in df.iterrows():\n",
        "    smiles = row['smiles']\n",
        "    temp1, temp2 = smiles.split('.')\n",
        "    if '+' in temp1:\n",
        "        df.at[i, 'smiles_cation'] = temp1\n",
        "        df.at[i, 'smiles_anion'] = temp2\n",
        "    else:\n",
        "        df.at[i, 'smiles_cation'] = temp2\n",
        "        df.at[i, 'smiles_anion'] = temp1\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "id": "b9CXe04ezB9s",
        "outputId": "1a4ccb48-8e78-4a1f-b1ae-ce8e0d1d578b"
      },
      "outputs": [],
      "source": [
        "df3 = pd.read_excel('./cosmo-predicted-hcs.xlsx')\n",
        "df3['ils'] = df3['IL_cation'] + ' ' + df3['IL_anion']\n",
        "\n",
        "# remove rows where henrycnodim == 0\n",
        "df3 = df3[df3['henrycnodim'] != 0]\n",
        "df3 = df3[df3['henryc'] != 0]\n",
        "df3['henryc'] = df3['henryc'] * 101325 # units alignment\n",
        "df3 = df3.reset_index(drop=True)\n",
        "\n",
        "target_column_cosmo = 'henryc'\n",
        "df3[target_column_cosmo] = 1 / (df3[target_column_cosmo])\n",
        "df3['smiles'] = df3['smiles_cation'] + '.' + df3['smiles_anion']\n",
        "df3['temps'] = 298\n",
        "df3['smiles_solutes'] = df3['task'] + '_cosmo'\n",
        "df3 = df3.dropna(subset=['smiles_anion'])\n",
        "df3 = df3.reset_index(drop=True)\n",
        "df3 = df3[['smiles', 'smiles_cation', 'smiles_anion', 'temps', target_column_cosmo, 'ils', 'smiles_solutes']]\n",
        "df3.rename(columns={target_column_cosmo: 'Hcs'}, inplace=True)\n",
        "q1 = df3['Hcs'].quantile(0.25)\n",
        "q3 = df3['Hcs'].quantile(0.75)\n",
        "iqr = q3 - q1\n",
        "df3 = df3[(df3['Hcs'] >= q1 - 1.5 * iqr) & (df3['Hcs'] <= q3 + 1.5 * iqr)]\n",
        "df3 = df3.reset_index(drop=True)\n",
        "\n",
        "scaler3 = StandardScaler()\n",
        "df3['Hcs'] = scaler3.fit_transform(df3['Hcs'].values.reshape(-1, 1))\n",
        "df = pd.concat([df, df3])\n",
        "df = df.reset_index(drop=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LULljdjU6qnq",
        "outputId": "7d187b76-1bcd-4818-8db8-58ab46bfef60"
      },
      "outputs": [],
      "source": [
        "nrm = rdMolStandardize.Normalizer()\n",
        "\n",
        "def normalize_smiles(smile):\n",
        "  cosmo_flag = False\n",
        "  if '_cosmo' in smile:\n",
        "    smile = smile.replace('_cosmo', '')\n",
        "    cosmo_flag = True\n",
        "  mol = Chem.MolFromSmiles(smile)\n",
        "  mol_norm = nrm.normalize(mol)\n",
        "  smile_norm = Chem.MolToSmiles(mol_norm, True)\n",
        "  if cosmo_flag:\n",
        "    smile_norm = smile_norm + '_cosmo'\n",
        "  return smile_norm\n",
        "\n",
        "smiles_cation_unique = df['smiles_cation'].unique()\n",
        "smiles_anion_unique = df['smiles_anion'].unique()\n",
        "\n",
        "smiles_cation_norm_dict = {}\n",
        "smiles_anion_norm_dict = {}\n",
        "for smile in smiles_cation_unique:\n",
        "  smiles_cation_norm_dict[smile] = normalize_smiles(smile)\n",
        "for smile in smiles_anion_unique:\n",
        "  smiles_anion_norm_dict[smile] = normalize_smiles(smile)\n",
        "\n",
        "df['smiles_cation'] = df['smiles_cation'].map(smiles_cation_norm_dict)\n",
        "df['smiles_anion'] = df['smiles_anion'].map(smiles_anion_norm_dict)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM62dgGavyOl"
      },
      "outputs": [],
      "source": [
        "tasks_names_to_test = ['O=C=O']\n",
        "\n",
        "# remove sufix '_valid' from samples in df\n",
        "df['smiles_solutes'] = df['smiles_solutes'].str.replace('_valid', '')\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "for task in tasks_names_to_test:\n",
        "  # randomly select number of percentage of samples with this task\n",
        "  df_task = df[df['smiles_solutes'] == task]\n",
        "\n",
        "  fold_to_use = 4\n",
        "  for fold_index, (train_index, valid_index) in enumerate(kf.split(df_task)):\n",
        "    if fold_index == fold_to_use:\n",
        "      df.loc[df_task.iloc[valid_index].index, 'smiles_solutes'] = task + f'_valid'\n",
        "      break  # Exit after marking the specified fold as validation\n",
        "\n",
        "# Extract SMILES, task numbers, and property values\n",
        "smiles = df['smiles'].values\n",
        "tasks = df['smiles_solutes'].values\n",
        "tasks_dict = {task: i for i, task in enumerate(np.unique(tasks))}\n",
        "tasks = np.array([tasks_dict[task] for task in tasks])\n",
        "\n",
        "y = df['Hcs'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEYUQWI6ujT-",
        "outputId": "ce70ba66-faec-4c9a-e046-64b0f059189a"
      },
      "outputs": [],
      "source": [
        "nBITS = 4096\n",
        "mfpgen_c = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=nBITS)\n",
        "ao_c = rdFingerprintGenerator.AdditionalOutput()\n",
        "ao_c.AllocateBitInfoMap()\n",
        "aos_c = []\n",
        "tpls_c = []\n",
        "mfpgen_a = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=nBITS)\n",
        "ao_a = rdFingerprintGenerator.AdditionalOutput()\n",
        "ao_a.AllocateBitInfoMap()\n",
        "aos_a = []\n",
        "tpls_a = []\n",
        "mfpgen_s = rdFingerprintGenerator.GetMorganGenerator(radius=2, fpSize=nBITS)\n",
        "ao_s = rdFingerprintGenerator.AdditionalOutput()\n",
        "ao_s.AllocateBitInfoMap()\n",
        "aos_s = []\n",
        "tpls_s = []\n",
        "\n",
        "fps_c, fps_a, fps_s = [], [], []\n",
        "\n",
        "for smi in df['smiles_cation'].values:\n",
        "  mol = MolFromSmiles(smi)\n",
        "  fp = mfpgen_c.GetFingerprintAsNumPy(mol, additionalOutput=ao_c)\n",
        "  mol_fp_info_map = ao_c.GetBitInfoMap()\n",
        "  aos_c.append(mol_fp_info_map)\n",
        "  for x in mol_fp_info_map.keys():\n",
        "    tpls_c.extend([(mol, x, mol_fp_info_map)])\n",
        "  fps_c.append(fp)\n",
        "for smi in df['smiles_anion'].values:\n",
        "  mol = MolFromSmiles(smi)\n",
        "  fp = mfpgen_a.GetFingerprintAsNumPy(mol, additionalOutput=ao_a)\n",
        "  mol_fp_info_map = ao_a.GetBitInfoMap()\n",
        "  aos_a.append(mol_fp_info_map)\n",
        "  for x in mol_fp_info_map.keys():\n",
        "    tpls_a.extend([(mol, x, mol_fp_info_map)])\n",
        "  fps_a.append(fp)\n",
        "for smi in df['smiles_solutes'].values:\n",
        "  if '_test' in smi or '_valid' in smi or '_cosmo' in smi:\n",
        "    smi = smi.replace('_test', '')\n",
        "    smi = smi.replace('_valid', '')\n",
        "    smi = smi.replace('_cosmo', '')\n",
        "  mol = MolFromSmiles(smi)\n",
        "  fp = mfpgen_s.GetFingerprintAsNumPy(mol, additionalOutput=ao_s)\n",
        "  mol_fp_info_map = ao_s.GetBitInfoMap()\n",
        "  aos_s.append(mol_fp_info_map)\n",
        "  for x in mol_fp_info_map.keys():\n",
        "    tpls_s.extend([(mol, x, mol_fp_info_map)])\n",
        "  fps_s.append(fp)\n",
        "\n",
        "tpls_c.sort(key = lambda i: i[1])\n",
        "tpls_c_reduced = [tpls_c[0]]\n",
        "for element in tpls_c:\n",
        "  if element[1] != tpls_c_reduced[-1][1]:\n",
        "    tpls_c_reduced.append(element)\n",
        "print(f\"{len(tpls_c_reduced) = }\")\n",
        "p_cations_fps = Draw.DrawMorganBits(tpls_c_reduced, molsPerRow=8, legends=['c_fp_' + str(x[1]) for x in tpls_c_reduced])\n",
        "p_cations_fps.save('./cations_fps.png')\n",
        "\n",
        "tpls_a.sort(key = lambda i: i[1])\n",
        "tpls_a_reduced = [tpls_a[0]]\n",
        "for element in tpls_a:\n",
        "  if element[1] != tpls_a_reduced[-1][1]:\n",
        "    tpls_a_reduced.append(element)\n",
        "print(f\"{len(tpls_a_reduced) = }\")\n",
        "p_anions_fps = Draw.DrawMorganBits(tpls_a_reduced, molsPerRow=8, legends=['a_fp_' + str(x[1]) for x in tpls_a_reduced])\n",
        "p_anions_fps.save('./anions_fps.png')\n",
        "\n",
        "fps_c_columns = np.array([f'c_fp_{i+1}' for i in range(nBITS)])\n",
        "fps_a_columns = np.array([f'a_fp_{i+1}' for i in range(nBITS)])\n",
        "fps_s_columns = np.array([f's_fp_{i+1}' for i in range(nBITS)])\n",
        "\n",
        "fps_c = np.array(fps_c, dtype=int)\n",
        "fps_a = np.array(fps_a, dtype=int)\n",
        "fps_s = np.array(fps_s, dtype=int)\n",
        "\n",
        "# Identify columns with zero variance\n",
        "non_zero_var_cols_c = np.var(fps_c, axis=0) != 0\n",
        "non_zero_var_cols_a = np.var(fps_a, axis=0) != 0\n",
        "non_zero_var_cols_s = np.var(fps_s, axis=0) != 0\n",
        "\n",
        "# Filter columns with non-zero variance\n",
        "fps_c_filtered = fps_c[:, non_zero_var_cols_c]\n",
        "fps_a_filtered = fps_a[:, non_zero_var_cols_a]\n",
        "fps_s_filtered = fps_s[:, non_zero_var_cols_s]\n",
        "fps_c_filtered_columns = fps_c_columns[non_zero_var_cols_c]\n",
        "fps_a_filtered_columns = fps_a_columns[non_zero_var_cols_a]\n",
        "fps_s_filtered_columns = fps_s_columns[non_zero_var_cols_s]\n",
        "# print(f'{fps_c_filtered.shape = }')\n",
        "# print(f'{fps_a_filtered.shape = }')\n",
        "# print(f'{fps_s_filtered.shape = }')\n",
        "\n",
        "# merge fps_c_filtered and fps_a_filtered\n",
        "# ecfp = np.concatenate((fps_c_filtered, fps_a_filtered), axis=1)\n",
        "ecfp = np.concatenate((fps_c_filtered, fps_a_filtered, fps_s_filtered), axis=1)\n",
        "print(f'{ecfp.shape = }')\n",
        "\n",
        "# columns_names_ecfp = np.concatenate((fps_c_filtered_columns, fps_a_filtered_columns))\n",
        "columns_names_ecfp = np.concatenate((fps_c_filtered_columns, fps_a_filtered_columns, fps_s_filtered_columns))\n",
        "print(f'{columns_names_ecfp.shape = }')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkzTkaMLsQcf"
      },
      "outputs": [],
      "source": [
        "dataset = dc.data.NumpyDataset(X=ecfp, y=np.array(y), ids=tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "eQ6VJcn7SkRF",
        "outputId": "89914226-5652-4fe1-d7ea-6434134a7e9a"
      },
      "outputs": [],
      "source": [
        "plt.hist(y, bins=100)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjaJB3CXSvSf",
        "outputId": "b54b6fb0-966e-4f7f-e19a-5ab54a6d45cc"
      },
      "outputs": [],
      "source": [
        "print(tasks_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AiOhdE0mEBE"
      },
      "outputs": [],
      "source": [
        "input_dim = dataset.X.shape[1]\n",
        "\n",
        "full_tasks_names_to_test = []\n",
        "for el in tasks_names_to_test:\n",
        "  full_tasks_names_to_test.append(el)\n",
        "  full_tasks_names_to_test.append(f\"{el}_valid\")\n",
        "  full_tasks_names_to_test.append(f\"{el}_test\")\n",
        "\n",
        "tasks_to_test = [tasks_dict[task] for task in full_tasks_names_to_test]\n",
        "\n",
        "learner = MetaMFLearner(\n",
        "    layer_sizes=[input_dim, 64, 32, 1],\n",
        "    dataset=dataset, batch_size=hiper_batch_size,\n",
        "    tasks_dict=tasks_dict, test_tasks=tasks_to_test\n",
        "    )\n",
        "optimizer = dc.models.optimizers.Adam(learning_rate=5e-3)\n",
        "maml = MAML(learner, meta_batch_size=hiper_batch_size, optimizer=optimizer)\n",
        "maml.fit(2500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "j93I0XSajE-d",
        "outputId": "42d5711d-bd0a-437e-fe7d-7ce92df5d10a"
      },
      "outputs": [],
      "source": [
        "for val_task in tasks_names_to_test:\n",
        "    print(f\"Testing on task: {val_task}\")\n",
        "    maml.restore()\n",
        "    learner.select_task_by_name(val_task)\n",
        "    batch = learner.get_batch()\n",
        "    print(f\"{batch[1].shape = }\")\n",
        "    loss, outputs = maml.predict_on_batch(batch)\n",
        "    print(loss)\n",
        "    maml.train_on_current_task(25, restore=False)\n",
        "    loss, outputs = maml.predict_on_batch(batch)\n",
        "    print(loss)\n",
        "    current_task_name = get_key_by_value(tasks_dict, learner.task_index)\n",
        "    print(f\"Task index: {learner.task_index}, task name: {current_task_name}\")\n",
        "\n",
        "    # validation set\n",
        "    indexes = df[df['smiles_solutes'] == f\"{val_task}_valid\"].index\n",
        "\n",
        "    y_true_val = y[indexes]\n",
        "    y_true_val = torch.tensor(y_true_val, dtype=torch.float32).view(-1, 1)\n",
        "    x_test_val = ecfp[indexes, :]\n",
        "    x_test_val = torch.tensor(x_test_val, dtype=torch.float32)\n",
        "\n",
        "    loss, outputs = maml.predict_on_batch([x_test_val, y_true_val])\n",
        "\n",
        "    y_pred_val = outputs[0].cpu().detach().numpy()\n",
        "    y_true_val = y_true_val.cpu().detach().numpy()\n",
        "\n",
        "    evaluator = RegressionMetric(y_true_val, y_pred_val)\n",
        "    list_metrics = [\"R2\", \"R2S\", \"A10\", \"RMSE\", \"MAE\", \"MAPE\", \"NSE\"]\n",
        "    print('Validation Set - Test Set')\n",
        "    for metric in list_metrics:\n",
        "        print(f\"{evaluator.get_metric_by_name(metric)[metric]: .4f}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "    # test set\n",
        "    indexes = df[df['smiles_solutes'] == f\"{val_task}_test\"].index\n",
        "\n",
        "    y_true = y[indexes]\n",
        "    y_true = torch.tensor(y_true, dtype=torch.float32).view(-1, 1)\n",
        "    x_test = ecfp[indexes, :]\n",
        "    x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "\n",
        "    loss, outputs = maml.predict_on_batch([x_test, y_true])\n",
        "\n",
        "    y_pred = outputs[0].cpu().detach().numpy()\n",
        "    y_true = y_true.cpu().detach().numpy()\n",
        "\n",
        "    evaluator = RegressionMetric(y_true, y_pred)\n",
        "    list_metrics = [\"R2\", \"R2S\", \"A10\", \"RMSE\", \"MAE\", \"MAPE\", \"NSE\"]\n",
        "    for metric in list_metrics:\n",
        "        print(f\"{evaluator.get_metric_by_name(metric)[metric]: .4f}\")\n",
        "\n",
        "    print()\n",
        "\n",
        "    for i in range(min(len(y_true), 10)):\n",
        "        print(f\"True: {float(y_true[i][0]): 3.4f} \\tPredicted: {float(y_pred[i][0]): 3.4f}\")\n",
        "\n",
        "    plt.scatter(y_true, y_pred, c='red')\n",
        "    plt.scatter(y_true_val, y_pred_val, c='blue')\n",
        "\n",
        "    plt.plot([np.min(y_true), np.max(y_true)], [np.min(y_true), np.max(y_true)], 'k--', lw=2)\n",
        "\n",
        "    plt.xlabel('True')\n",
        "    plt.ylabel('Predicted')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSVLj3b6bVma"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
